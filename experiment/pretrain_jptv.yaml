dataset:
  name: wikitext-pretrain
  path: datasets/TextClassification/wikitext

plm:
  model_name: roberta
  model_path: roberta-large
  optimize:
    freeze_para: True
    lr: 0.00003
    weight_decay: 0.01
    scheduler:
      type: 
      num_warmup_steps: 500

checkpoint:
  save_latest: False
  save_best: False

train:
  batch_size: 8 # ====== 8
  num_epochs: 0
  train_verblizer: post
  clean: True
  shuffle_data: False

test:
  batch_size: 64

template: mixed_template
verbalizer: jptv_verbalizer

manual_template:
  choice: 0
  file_path: scripts/TextClassification/agnews/manual_template.txt

soft_verbalizer:
  parent_config: verbalizer
  choice: 0
  file_path: scripts/TextClassification/agnews/manual_verbalizer.txt
  num_classes: 4
  random_init: true
  optimize:
    lr: 0.001
    weight_decay: 0.0
    scheduler:
      num_warmup_steps: 0
  
logging:
  path_base: logs
  unique_string: weight_nsp_64

mixed_template:
  choice: 0
  file_path: scripts/TextClassification/agnews/mixed_template.txt
  optimize:
    lr: 0.003
    weight_decay: 0.0
    scheduler:
      num_warmup_steps: 0

jptv_verbalizer:
  parent_config: verbalizer
  choice: 0
  file_path: scripts/TextClassification/agnews/manual_verbalizer.txt
  lr: 0.01
  mid_dim: 64
  epochs: 30
  pretrain: true

environment:
  num_gpus: 1
  cuda_visible_devices:
    - 0
  local_rank: 0
  
learning_setting: full # =================

reproduce:  # seed for reproduction 
  seed: 123  # a seed for all random part
